{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-11T09:43:53.660957Z",
     "start_time": "2021-12-11T09:43:50.363861Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "from statistics import mean\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.special import comb\n",
    "from scipy import interp\n",
    "import scipy\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from noesis import Noesis\n",
    "import logging\n",
    "import warnings\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.collections import LineCollection\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm, LinearSegmentedColormap\n",
    "\n",
    "sns.set_theme()\n",
    "sns.set_style(\"whitegrid\", {\"grid.color\": \".6\", \"grid.linestyle\": \":\"})\n",
    "\n",
    "logging.getLogger('matplotlib.font_manager').disabled = True\n",
    "logging.getLogger('matplotlib.axes').disabled = True\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "tqdm.pandas()\n",
    "\n",
    "sys.path.append(\"../src/models/\")\n",
    "\n",
    "\n",
    "#import mutual_information\n",
    "#import katz\n",
    "#import random_walk\n",
    "import tgn\n",
    "import jodie\n",
    "import dyrep\n",
    "import tgn_viz\n",
    "data_path = \"../data/processed\"\n",
    "\n",
    "ns = Noesis()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_dataset(data_name):\n",
    "    train = pd.read_csv(os.path.join(data_path, f\"{data_name}/train/ml_{data_name}.csv\"), index_col=0)\n",
    "    test = pd.read_csv(os.path.join(data_path, f\"{data_name}/test/ml_{data_name}.csv\"), index_col=0)\n",
    "    # CM and Jaccard constant 0, beacuse it is a bipartite graph\n",
    "    \n",
    "    pred, embed = tgn.predict(data_name, \"tgn_ablation_time\", ablation='time', seed=0, n_runs=5, n_epoch=10)\n",
    "    for i, val in enumerate(pred):\n",
    "        test[f\"tgn_ablation_time_{i}\"] = val\n",
    "    \n",
    "    pred, embed = jodie.predict(data_name, \"jodie_ablation_time\", ablation='time', seed=0, n_runs=5, n_epoch=10)\n",
    "    for i, val in enumerate(pred):\n",
    "        test[f\"jodie_ablation_time{i}\"] = val\n",
    "        \n",
    "    pred, embed = dyrep.predict(data_name, \"dyrep_ablation_time\", ablation='time', seed=0, n_runs=5, n_epoch=10)\n",
    "    for i, val in enumerate(pred):\n",
    "        test[f\"dyrep_ablation_time{i}\"] = val\n",
    "        \n",
    "    pred, embed = tgn.predict(data_name, \"tgn\", seed=0, n_runs=5, n_epoch=10)\n",
    "    for i, val in enumerate(pred):\n",
    "        test[f\"tgn_{i}\"] = val\n",
    "    \n",
    "    pred, embed = jodie.predict(data_name, \"jodie\", seed=0, n_runs=5, n_epoch=10)\n",
    "    for i, val in enumerate(pred):\n",
    "        test[f\"jodie_{i}\"] = val\n",
    "        \n",
    "    pred, embed = dyrep.predict(data_name, \"dyrep\", seed=0, n_runs=5, n_epoch=10)\n",
    "    for i, val in enumerate(pred):\n",
    "        test[f\"dyrep_{i}\"] = val\n",
    "        \n",
    "    test[\"mutual\"], test[\"mutual_normalized\"] = mutual_information.predict(train, test, normalize=True)\n",
    "    test['katz'] = katz.predict(os.path.join(data_path, f\"{data_name}/train/{data_name}_train.json\"),\n",
    "                            os.path.join(data_path, f\"{data_name}/train/{data_name}_train.gml\"),\n",
    "                            test,\n",
    "                            ns)\n",
    "    res = random_walk.predict(os.path.join(data_path, f\"{data_name}/train/{data_name}_train.json\"),\n",
    "                            os.path.join(data_path, f\"{data_name}/train/{data_name}_train.gml\"),\n",
    "                            test,\n",
    "                            ns)\n",
    "    for i, val in enumerate(res):\n",
    "        test[f\"random_walk_{i}\"] = val\n",
    "    \n",
    "    test.to_csv(f\"../data/results/test_{data_name}.csv\")\n",
    "    \n",
    "    return test\n",
    "\n",
    "def plot_results(test):\n",
    "    col_names = {\"Mutual Information\": [\"mutual_normalized\"],\n",
    "                \"Random Walk\": [f\"random_walk_{x}\" for x in range(5)],\n",
    "                \"TGN (ablate timestamps)\": [f\"tgn_ablation_time_{x}\" for x in range(5)],\n",
    "                \"TGN\": [f\"tgn_{x}\" for x in range(5)],\n",
    "                \"Jodie (ablate timestamps)\": [f\"jodie_ablation_time{x}\" for x in range(5)],\n",
    "                \"Jodie\": [f\"jodie_{x}\" for x in range(5)],\n",
    "                \"DyRep (ablate timestamps)\": [f\"dyrep_ablation_time{x}\" for x in range(5)],\n",
    "                \"DyRep\": [f\"dyrep_{x}\" for x in range(5)]}\n",
    "    \n",
    "    \n",
    "    label = test.ground_truth.values\n",
    "    for title, col in col_names.items():\n",
    "        tprs = []\n",
    "        base_fpr = np.linspace(0, 1, 101)\n",
    "        auc=[]\n",
    "        for pred in col:\n",
    "            pred_prob = test[pred].values\n",
    "            fpr, tpr, thresh = metrics.roc_curve(label,\n",
    "                                                 pred_prob)\n",
    "            plt.plot(fpr, tpr, color=[52/255,97/255,120/255], alpha=0.15)\n",
    "            auc.append(metrics.roc_auc_score(label, pred_prob))\n",
    "            \n",
    "            tpr = interp(base_fpr, fpr, tpr)\n",
    "            tpr[0] = 0.0\n",
    "            tprs.append(tpr)\n",
    "        tprs = np.array(tprs)\n",
    "        mean_tprs = tprs.mean(axis=0)\n",
    "        std = tprs.std(axis=0)\n",
    "            \n",
    "        tprs_upper = np.minimum(mean_tprs + std, 1)\n",
    "        tprs_lower = mean_tprs - std\n",
    "        \n",
    "        plt.plot(base_fpr, mean_tprs, color=[52/255,97/255,120/255], lw=2)\n",
    "        plt.fill_between(base_fpr, tprs_lower, tprs_upper, color=[143/255,195/255,216/255], alpha=0.3)\n",
    "\n",
    "        plt.plot([0, 1], [0, 1],'--', color = [252/255,97/255,31/255])\n",
    "        plt.xlim([-0.01, 1.01])\n",
    "        plt.ylim([-0.01, 1.01])\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        # plt.axes().set_aspect('equal', 'datalim')\n",
    "        plt.title(f\"{title} ROC curve, AUC: {mean(auc):.4f}\")\n",
    "        plt.show()\n",
    "\n",
    "        # PR curve\n",
    "        tprs = []\n",
    "        base_fpr = np.linspace(0, 1, 101)\n",
    "        auc=[]\n",
    "        for pred in col:\n",
    "            pred_prob = test[pred].values\n",
    "            precision, recall, thresholds = metrics.precision_recall_curve(label, pred_prob)\n",
    "            auc.append(metrics.auc(recall, precision))\n",
    "            \n",
    "            plt.plot(recall, precision, color=[52/255,97/255,120/255], alpha=0.15)\n",
    "            \n",
    "            reversed_recall = np.fliplr([recall])[0]\n",
    "            reversed_precision = np.fliplr([precision])[0]\n",
    "            tpr = interp(base_fpr, reversed_recall, reversed_precision)\n",
    "            tpr[0] = 1.0\n",
    "            tprs.append(tpr)\n",
    "        tprs = np.array(tprs)\n",
    "        mean_tprs = tprs.mean(axis=0)\n",
    "        std = tprs.std(axis=0)\n",
    "            \n",
    "        tprs_upper = np.minimum(mean_tprs + std, 1)\n",
    "        tprs_lower = mean_tprs - std\n",
    "        \n",
    "        plt.plot(base_fpr, mean_tprs, color=[52/255,97/255,120/255], lw=2)\n",
    "        plt.fill_between(base_fpr, tprs_lower, tprs_upper, color=[143/255,195/255,216/255], alpha=0.3)\n",
    "        plt.plot([0, 1], [0.5, 0.5],'--', color = [252/255,97/255,31/255])\n",
    "        plt.xlim([-0.01, 1.01])\n",
    "        plt.ylim([-0.01, 1.01])\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.title(f\"{title} PR curve, AUC: {mean(auc):.4f}\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = predict_dataset('lastfm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-08T19:04:32.396654Z",
     "start_time": "2021-12-07T23:12:21.969363Z"
    }
   },
   "outputs": [],
   "source": [
    "test = predict_dataset('mooc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-09T05:10:29.316489Z",
     "start_time": "2021-12-08T20:15:37.530658Z"
    }
   },
   "outputs": [],
   "source": [
    "test = predict_dataset('reddit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-08T19:04:33.512691Z",
     "start_time": "2021-12-08T19:04:33.512691Z"
    }
   },
   "outputs": [],
   "source": [
    "test = predict_dataset('wikipedia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-07T21:09:05.060831Z",
     "start_time": "2021-12-07T21:09:05.060831Z"
    }
   },
   "outputs": [],
   "source": [
    "ns.end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time split dataset prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-11T09:43:57.162634Z",
     "start_time": "2021-12-11T09:43:57.141634Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_last_models(path):\n",
    "    df = pd.DataFrame([x.split('-') for x in os.listdir(path)], columns = [\"folder\", \"model\", \"run\", \"epoch\"])\n",
    "    df_g = df.groupby([\"folder\", \"model\", \"run\"]).max().reset_index()\n",
    "    files = {int(y):os.path.join(path, '-'.join(x)) for x,y in zip(df_g.values, df_g.run.values)}\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T06:56:32.843445Z",
     "start_time": "2021-12-14T06:56:32.780340Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_split(dataset_name):\n",
    "    path = f\"../data/processed/split_data/{dataset_name}/\"\n",
    "    for folder in os.listdir(path):\n",
    "        datapath = os.path.join(path,folder)\n",
    "        print(datapath, folder)\n",
    "        n_split = int(folder[-1])\n",
    "        if n_split > 0:\n",
    "            old_folder = f\"{dataset_name}_{n_split-1}\"\n",
    "        #pred, embed = tgn.predict(folder, \"tgn_ablation_time\", ablation='time', seed=0, n_runs=5, n_epoch=10, data_path = datapath)\n",
    "        #pred, embed = jodie.predict(folder, \"jodie_ablation_time\", ablation='time', seed=0, n_runs=5, n_epoch=10, data_path = datapath)\n",
    "        #pred, embed = dyrep.predict(folder, \"dyrep_ablation_time\", ablation='time', seed=0, n_runs=5, n_epoch=10, data_path = datapath)\n",
    "        if not os.path.isdir(f\"../data/results/{folder}/tgn\"):\n",
    "            if n_split >0:\n",
    "                models_to_load = get_last_models(f\"../models/{old_folder}/tgn/saved_checkpoints\")\n",
    "            else:\n",
    "                models_to_load = None\n",
    "            print(\"Train model TGN...\")\n",
    "            pred, embed = tgn.predict(folder, \"tgn\", seed=11,\n",
    "                                      n_runs=3, n_epoch=10,\n",
    "                                      data_path = datapath,\n",
    "                                      models_to_load=models_to_load)\n",
    "            \n",
    "        if not os.path.isdir(f\"../data/results/{folder}/jodie\"):\n",
    "            if n_split >0:\n",
    "                models_to_load = get_last_models(f\"../models/{old_folder}/jodie/saved_checkpoints\")\n",
    "            else:\n",
    "                models_to_load = None\n",
    "            print(\"Train model Jodie...\")\n",
    "            pred, embed = jodie.predict(folder, \"jodie\",\n",
    "                                        seed=112, n_runs=3, n_epoch=10,\n",
    "                                        data_path = datapath,\n",
    "                                        models_to_load=models_to_load)\n",
    "            \n",
    "        if not os.path.isdir(f\"../data/results/{folder}/dyrep\"):\n",
    "            if n_split >0:\n",
    "                models_to_load = get_last_models(f\"../models/{old_folder}/dyrep/saved_checkpoints\")\n",
    "            else:\n",
    "                models_to_load = None\n",
    "            print(\"Train model DyRep...\")\n",
    "            pred, embed = dyrep.predict(folder, \"dyrep\",\n",
    "                                        seed=112, n_runs=3, n_epoch=10,\n",
    "                                        data_path = datapath,\n",
    "                                        models_to_load=models_to_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T07:07:44.506652Z",
     "start_time": "2021-12-14T06:56:33.261453Z"
    }
   },
   "outputs": [],
   "source": [
    "for dataset_name in ['wikipedia', 'reddit', 'mooc', 'lastfm']:\n",
    "    predict_split(dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TGN visualization training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inter_from_256(x):\n",
    "    return np.interp(x=x,xp=[0,255],fp=[0,1])\n",
    "\n",
    "rgb_list = [[52,97,120], [92, 166, 154], [252,97,31], [253, 193, 14]]\n",
    "all_red = []\n",
    "all_green = []\n",
    "all_blue = []\n",
    "for rgb in rgb_list:\n",
    "    all_red.append(rgb[0])\n",
    "    all_green.append(rgb[1])\n",
    "    all_blue.append(rgb[2])\n",
    "# build each section\n",
    "n_section = len(all_red) - 1\n",
    "red = tuple([(1/n_section*i,inter_from_256(v),inter_from_256(v)) for i,v in enumerate(all_red)])\n",
    "green = tuple([(1/n_section*i,inter_from_256(v),inter_from_256(v)) for i,v in enumerate(all_green)])\n",
    "blue = tuple([(1/n_section*i,inter_from_256(v),inter_from_256(v)) for i,v in enumerate(all_blue)])\n",
    "cdict = {'red':red,'green':green,'blue':blue}\n",
    "new_cmap = LinearSegmentedColormap('new_cmap',segmentdata=cdict)\n",
    "\n",
    "\n",
    "def plot_node_mvmt(x,y,t):\n",
    "    points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "    segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Create a continuous norm to map from data points to colors\n",
    "    norm = plt.Normalize(t.min(), t.max())\n",
    "    lc = LineCollection(segments, cmap=new_cmap, norm=norm, alpha=0.3)\n",
    "    # Set the values used for colormapping\n",
    "    lc.set_array(t)\n",
    "    lc.set_linewidth(1)\n",
    "    line = ax.add_collection(lc)\n",
    "    fig.colorbar(line, ax=ax)\n",
    "    # plt.scatter(x,y)\n",
    "\n",
    "    sns.scatterplot(x=x, y=y, c=t, cmap=new_cmap, alpha=0.5)\n",
    "\n",
    "    height = y.max() - y.min()\n",
    "    width = x.max() - x.min()\n",
    "\n",
    "    plt.xlim([x.min() - 0.1*width, x.max() + 0.1*width])\n",
    "    plt.ylim([y.min() - 0.1*height, y.max() + 0.1*height])\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def calc_and_viz(data_name, test, affinity_merge_layer, node=7144):\n",
    "    print(affinity_merge_layer.upper())\n",
    "    \n",
    "    res = tgn_viz.predict(data_name, f\"tgn_viz_{affinity_merge_layer}\", seed=0, n_runs=1, n_epoch=10, affinity_merge_layer=affinity_merge_layer)\n",
    "    preds, nodes, times, embeds = res\n",
    "    for i, val in enumerate(preds):\n",
    "        test[f\"tgn_viz_{affinity_merge_layer}_{i}\"] = val\n",
    "\n",
    "    sc = StandardScaler()\n",
    "    sc.fit(embeds)\n",
    "    X_train_std = sc.transform(embeds)\n",
    "    pca = PCA(n_components=2)\n",
    "    embed_pca = pca.fit_transform(X_train_std)\n",
    "\n",
    "    for n in [2134, 7066, 3958, 7058, 4309, 6419]:\n",
    "        print(n)\n",
    "        try:\n",
    "            mask = nodes == n\n",
    "            plot_node_mvmt(embeds[mask,0],embeds[mask,1],times[mask])\n",
    "            plot_node_mvmt(embed_pca[mask,0],embed_pca[mask,1],times[mask])\n",
    "        except:\n",
    "            continue\n",
    "    return test\n",
    "\n",
    "def predict_viz_dataset(data_name, node=7144):\n",
    "    train = pd.read_csv(os.path.join(data_path, f\"{data_name}/train/ml_{data_name}.csv\"), index_col=0)\n",
    "    test = pd.read_csv(os.path.join(data_path, f\"{data_name}/test/ml_{data_name}.csv\"), index_col=0)\n",
    "    \n",
    "    # test = calc_and_viz(data_name, test, \"default\", node=node)\n",
    "\n",
    "    test = calc_and_viz(data_name, test, \"extra_layers\", node=node) # 0.5407455559081602\n",
    "    test = calc_and_viz(data_name, test, \"extra_layers_had\", node=node) # 0.7102923331217205\n",
    "    test = calc_and_viz(data_name, test, \"extra_layers_sincos\", node=node) # 0.5\n",
    "    test = calc_and_viz(data_name, test, \"extra_layers_extra6\", node=node) # 0.7357540433690669\n",
    "    test = calc_and_viz(data_name, test, \"extra_layers_extra6_sincoshad\", node=node) # 0.5464958168453279\n",
    "\n",
    "    test = calc_and_viz(data_name, test, \"extra_layers_relu\", node=node) # 0.3875494501740585\n",
    "    test = calc_and_viz(data_name, test, \"extra_layers_had_relu\", node=node) # 0.5107732625871076\n",
    "    test = calc_and_viz(data_name, test, \"extra_layers_sincos_relu\", node=node) # 0.5688160155678049\n",
    "    test = calc_and_viz(data_name, test, \"extra_layers_extra6_relu\", node=node) # 0.5718021841862889\n",
    "    test = calc_and_viz(data_name, test, \"extra_layers_extra6_sincoshad_relu\", node=node) # 0.415627888526674\n",
    "\n",
    "    test = calc_and_viz(data_name, test, \"extra_layers_tanh\", node=node) # 0.4294699816102592\n",
    "    test = calc_and_viz(data_name, test, \"extra_layers_had_tanh\", node=node) # 0.8038525607161809\n",
    "    test = calc_and_viz(data_name, test, \"extra_layers_sincos_tanh\", node=node) # 0.5\n",
    "    test = calc_and_viz(data_name, test, \"extra_layers_extra6_tanh\", node=node) # 0.4583873437927973\n",
    "    test = calc_and_viz(data_name, test, \"extra_layers_extra6_sincoshad_tanh\", node=node) # 0.6201307368437561\n",
    "\n",
    "    test = calc_and_viz(data_name, test, \"extra_layers_sigmoid\", node=node) # 0.3564450016425934\n",
    "    test = calc_and_viz(data_name, test, \"extra_layers_had_sigmoid\", node=node) # 0.37128277873516746\n",
    "    test = calc_and_viz(data_name, test, \"extra_layers_sincos_sigmoid\", node=node) # 0.4470434151774143\n",
    "    test = calc_and_viz(data_name, test, \"extra_layers_extra6_sigmoid\", node=node) # 0.5001662306099629\n",
    "    test = calc_and_viz(data_name, test, \"extra_layers_extra6_sincoshad_sigmoid\", node=node) # \n",
    "\n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = predict_viz_dataset('mooc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "428.8px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
