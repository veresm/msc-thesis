{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d22b0c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-10T13:11:53.234042Z",
     "start_time": "2021-12-10T13:11:53.167043Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import interp\n",
    "import networkx as nx\n",
    "from sklearn import metrics\n",
    "from statistics import mean\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import logging\n",
    "import warnings\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import textwrap\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.collections import LineCollection\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm, LinearSegmentedColormap\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib.cm import ScalarMappable\n",
    "\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "import tabulate\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../src/models/\")\n",
    "import tgn_viz\n",
    "import tgn\n",
    "from src.tgn_viz.utils.data_processing import Data\n",
    "\n",
    "logging.getLogger('matplotlib.font_manager').disabled = True\n",
    "\n",
    "sns.set_theme()\n",
    "sns.set_style(\"whitegrid\", {\"grid.color\": \".6\", \"grid.linestyle\": \":\"})\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "colors = [\"#346178\",  #blue 52,97,120\n",
    "          \"#FC611F\", #orange 252,97,31\n",
    "          \"#5C6547\", #green\n",
    "         \"#FDC10E\", #yellow\n",
    "         \"#8EC3D9\", #light blue\n",
    "         \"#EA99A2\", # pink\n",
    "         \"#A65C7B\", \n",
    "         \"#5CA69A\" # teal\n",
    "         ]\n",
    "customPalette = sns.set_palette(sns.color_palette(colors))\n",
    "\n",
    "def inter_from_256(x):\n",
    "    return np.interp(x=x,xp=[0,255],fp=[0,1])\n",
    "\n",
    "rgb_list = [[52,97,120], [92, 166, 154], [252,97,31], [253, 193, 14]]\n",
    "all_red = []\n",
    "all_green = []\n",
    "all_blue = []\n",
    "for rgb in rgb_list:\n",
    "    all_red.append(rgb[0])\n",
    "    all_green.append(rgb[1])\n",
    "    all_blue.append(rgb[2])\n",
    "# build each section\n",
    "n_section = len(all_red) - 1\n",
    "red = tuple([(1/n_section*i,inter_from_256(v),inter_from_256(v)) for i,v in enumerate(all_red)])\n",
    "green = tuple([(1/n_section*i,inter_from_256(v),inter_from_256(v)) for i,v in enumerate(all_green)])\n",
    "blue = tuple([(1/n_section*i,inter_from_256(v),inter_from_256(v)) for i,v in enumerate(all_blue)])\n",
    "cdict = {'red':red,'green':green,'blue':blue}\n",
    "new_cmap = LinearSegmentedColormap('new_cmap',segmentdata=cdict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325c45d4",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# General statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d31fd91",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-09T15:51:14.558044Z",
     "start_time": "2021-12-09T15:51:14.358046Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def degree_distribution(dataset_name):\n",
    "    filename = dataset_name.lower()\n",
    "    path = f\"../data/interim/{filename}/ml_{filename}.csv\"\n",
    "    df = pd.read_csv(path, index_col=0)\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(set(df.u.tolist()), bipartite=0)\n",
    "    G.add_nodes_from(set(df.i.tolist()), bipartite=1)\n",
    "    G.add_edges_from(df[['u', 'i']].values)\n",
    "\n",
    "    A = set(df.u.tolist())\n",
    "    B = set(df.i.tolist())\n",
    "    plt.figure(figsize=(10, 7)) \n",
    "\n",
    "    degrees_A = [G.degree()[x] for x in A]\n",
    "    counts_a = Counter(degrees_A)\n",
    "    hist_a = [counts_a.get(i, 0) for i in range(max(counts_a) + 1)]\n",
    "    degrees = range(len(hist_a))\n",
    "    plt.loglog(degrees, hist_a,'o-', color=[52/255,97/255,120/255], alpha=0.7) \n",
    "\n",
    "    degrees_B = [G.degree()[x] for x in B]\n",
    "    counts_b = Counter(degrees_B)\n",
    "    hist_b = [counts_b.get(i, 0) for i in range(max(counts_b) + 1)]\n",
    "    degrees = range(len(hist_b))\n",
    "    plt.loglog(degrees, hist_b,'o-', color=[252/255,97/255,31/255], alpha=0.7) \n",
    "\n",
    "    plt.xlabel('Degree', fontsize=15)\n",
    "    plt.ylabel('Frequency', fontsize=15)\n",
    "    plt.legend([\"user\", \"item\"], prop={'size': 15})\n",
    "    plt.title(f\"Degree distribution of {dataset_name}\", fontsize=15)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a4b99f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T20:38:43.505955Z",
     "start_time": "2021-12-01T20:38:17.110324Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "names = ['LastFM', 'MOOC', 'Reddit', 'Wikipedia']\n",
    "for name in names:\n",
    "    degree_distribution(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88cffe4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-08T09:41:53.448025Z",
     "start_time": "2021-12-08T09:41:53.406025Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# distinct degrees in time\n",
    "def distinct_degrees_time_plot(dataset_name):\n",
    "    alphas = {'LastFM':[0.02,0.3,0.02, 0.3],\n",
    "              'MOOC':[0.05,0.3,0.005, 0.3],\n",
    "              'Reddit':[0.02,0.3,0.015, 0.4],\n",
    "              'Wikipedia':[0.02,0.3,0.015, 0.4]}\n",
    "    \n",
    "    filename = dataset_name.lower()\n",
    "    path = f\"../data/interim/{filename}/ml_{filename}.csv\"\n",
    "    df = pd.read_csv(path, index_col=0)\n",
    "\n",
    "    fig, [ax_1, ax_2] = plt.subplots(1,2, figsize=(12,4))\n",
    "\n",
    "    base_points = np.linspace(df.ts.min(), df.ts.max(), 1001)\n",
    "\n",
    "    funs_b = []\n",
    "    b = set(df.i.tolist())\n",
    "    for user in tqdm(b):\n",
    "        d = df[df.i == user].drop_duplicates(subset=['u','i'], keep='first', ignore_index=True).reset_index()[['index', 'ts']]\n",
    "        if d.shape[0] > 0:\n",
    "            ax_1.plot(d.ts, d.index, alpha=alphas[dataset_name][0], color=[252/255,97/255,31/255])\n",
    "            fun = np.interp(base_points, d.ts.values, d.index.values)\n",
    "            fun[0] = 0.0\n",
    "            funs_b.append(fun)\n",
    "\n",
    "    funs_b = np.array(funs_b)\n",
    "    mean_funs_b = funs_b.mean(axis=0)\n",
    "    std = funs_b.std(axis=0)\n",
    "\n",
    "    funs_b_upper = mean_funs_b + std\n",
    "    funs_b_lower = np.maximum(mean_funs_b - std,0)\n",
    "\n",
    "    ax_1.plot(base_points, mean_funs_b, color=[206/255,91/255,45/255], lw=2)\n",
    "    ax_1.fill_between(base_points, funs_b_lower, funs_b_upper, color=[252/255,97/255,31/255], alpha=alphas[dataset_name][1])\n",
    "\n",
    "    ax_1.set_title(f\"Degree development of item nodes\\n ({dataset_name})\", fontsize=14)\n",
    "    #######################\n",
    "    funs_a=[]\n",
    "    a = set(df.u.tolist())\n",
    "\n",
    "    for user in tqdm(a):\n",
    "        d = df[df.u == user].drop_duplicates(subset=['u','i'], keep='first', ignore_index=True).reset_index()[['index', 'ts']]\n",
    "        if d.shape[0] > 0:\n",
    "            ax_2.plot(d.ts, d.index, alpha=alphas[dataset_name][2], color=[52/255,97/255,120/255])\n",
    "            fun = np.interp(base_points, d.ts.values, d.index.values)\n",
    "            fun[0] = 0.0\n",
    "            funs_a.append(fun)\n",
    "\n",
    "    funs_a = np.array(funs_a)\n",
    "    mean_funs_a = funs_a.mean(axis=0)\n",
    "    std = funs_a.std(axis=0)\n",
    "\n",
    "    funs_a_upper = mean_funs_a + std\n",
    "    funs_a_lower = np.maximum(mean_funs_a - std,0)\n",
    "\n",
    "    ax_2.plot(base_points, mean_funs_a, color=[52/255,97/255,120/255], lw=2)\n",
    "    ax_2.fill_between(base_points, funs_a_lower, funs_a_upper, color=[143/255,195/255,216/255], alpha=alphas[dataset_name][3])\n",
    "\n",
    "    ax_2.set_title(f\"Degree development of user nodes\\n ({dataset_name})\", fontsize=14)\n",
    "\n",
    "    plt.savefig(f\"../reports/figures/distinct_degrees_time_plot_{dataset_name}.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54610d9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-08T09:45:41.333025Z",
     "start_time": "2021-12-08T09:41:54.743025Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "names = ['LastFM', 'MOOC', 'Reddit', 'Wikipedia']\n",
    "for name in names:\n",
    "    distinct_degrees_time_plot(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9b1129",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-08T09:45:41.658025Z",
     "start_time": "2021-12-08T09:45:41.631025Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# distinct degrees in time\n",
    "def degrees_time_plot(dataset_name):\n",
    "    alphas = {'LastFM':[0.025,0.3,0.04, 0.3],\n",
    "              'MOOC':[0.06,0.3,0.01, 0.4],\n",
    "              'Reddit':[0.03,0.3,0.02, 0.4],\n",
    "              'Wikipedia':[0.03,0.3,0.02, 0.4]}\n",
    "    \n",
    "    filename = dataset_name.lower()\n",
    "    path = f\"../data/interim/{filename}/ml_{filename}.csv\"\n",
    "    df = pd.read_csv(path, index_col=0)\n",
    "\n",
    "    fig, [ax_1, ax_2] = plt.subplots(1,2, figsize=(12,4))\n",
    "\n",
    "    base_points = np.linspace(df.ts.min(), df.ts.max(), 1001)\n",
    "\n",
    "    funs_b = []\n",
    "    b = set(df.i.tolist())\n",
    "    for user in tqdm(b):\n",
    "        d = df[df.i == user].reset_index()[['index', 'ts']]\n",
    "        if d.shape[0] > 0:\n",
    "            ax_1.plot(d.ts, d.index, alpha=alphas[dataset_name][0], color=[252/255,97/255,31/255])\n",
    "            fun = np.interp(base_points, d.ts.values, d.index.values)\n",
    "            fun[0] = 0.0\n",
    "            funs_b.append(fun)\n",
    "\n",
    "    funs_b = np.array(funs_b)\n",
    "    mean_funs_b = funs_b.mean(axis=0)\n",
    "    std = funs_b.std(axis=0)\n",
    "\n",
    "    funs_b_upper = mean_funs_b + std\n",
    "    funs_b_lower = np.maximum(mean_funs_b - std,0)\n",
    "\n",
    "    ax_1.plot(base_points, mean_funs_b, color=[206/255,91/255,45/255], lw=2)\n",
    "    ax_1.fill_between(base_points, funs_b_lower, funs_b_upper, color=[252/255,97/255,31/255], alpha=alphas[dataset_name][1])\n",
    "\n",
    "    ax_1.set_title(f\"Degree development of item nodes\\n ({dataset_name}, including duplicate edges)\", fontsize=14)\n",
    "    #######################\n",
    "    funs_a=[]\n",
    "    a = set(df.u.tolist())\n",
    "\n",
    "    for user in tqdm(a):\n",
    "        d = df[df.u == user].reset_index()[['index', 'ts']]\n",
    "        if d.shape[0] > 0:\n",
    "            ax_2.plot(d.ts, d.index, alpha=alphas[dataset_name][2], color=[52/255,97/255,120/255])\n",
    "            fun = np.interp(base_points, d.ts.values, d.index.values)\n",
    "            fun[0] = 0.0\n",
    "            funs_a.append(fun)\n",
    "\n",
    "    funs_a = np.array(funs_a)\n",
    "    mean_funs_a = funs_a.mean(axis=0)\n",
    "    std = funs_a.std(axis=0)\n",
    "\n",
    "    funs_a_upper = mean_funs_a + std\n",
    "    funs_a_lower = np.maximum(mean_funs_a - std,0)\n",
    "\n",
    "    ax_2.plot(base_points, mean_funs_a, color=[52/255,97/255,120/255], lw=2)\n",
    "    ax_2.fill_between(base_points, funs_a_lower, funs_a_upper, color=[143/255,195/255,216/255], alpha=alphas[dataset_name][3])\n",
    "\n",
    "    ax_2.set_title(f\"Degree development of user nodes\\n ({dataset_name}, all edges)\", fontsize=14)\n",
    "\n",
    "    plt.savefig(f\"../reports/figures/degrees_time_plot_{dataset_name}.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604425b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-08T09:48:39.370039Z",
     "start_time": "2021-12-08T09:45:41.959026Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "names = ['LastFM', 'MOOC', 'Reddit', 'Wikipedia']\n",
    "for name in names:\n",
    "    degrees_time_plot(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bdbac6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# number of nodes/edges, unique edges/nodes, edges, density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb371b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-08T08:28:12.436085Z",
     "start_time": "2021-12-08T08:28:12.383083Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def graph_statistics(dataset_name):\n",
    "    filename = dataset_name.lower()\n",
    "    path = f\"../data/interim/{filename}/ml_{filename}.csv\"\n",
    "    df = pd.read_csv(path, index_col=0)\n",
    "\n",
    "    unique_edges = df[['u', 'i']].drop_duplicates().shape[0]\n",
    "    all_edges = df[['u', 'i']].shape[0]\n",
    "    unique_nodes = df.u.append(df.i).unique().shape[0]\n",
    "    unique_user_nodes = df.u.unique().shape[0]\n",
    "    unique_item_nodes = df.i.unique().shape[0]\n",
    "\n",
    "    average_edge_duplication = all_edges / unique_edges\n",
    "\n",
    "    all_possible_edges = unique_user_nodes * unique_item_nodes\n",
    "    \n",
    "    edge_repetition = 1 - (unique_edges/all_edges)\n",
    "\n",
    "    density = unique_edges / all_possible_edges\n",
    "\n",
    "    table = [[f\"{dataset_name} statistics\"],\n",
    "             [\"Nodes\", \"User nodes\", \"Item nodes\", \"Unique edges\",\n",
    "              \"All edges\",\"Edge repetition\", \"Avergae edge duplication ratio\", \"All possible edges\", \"Density\"],\n",
    "             [unique_nodes,unique_user_nodes,unique_item_nodes, unique_edges,\n",
    "              all_edges,edge_repetition,average_edge_duplication, all_possible_edges,density]]\n",
    "\n",
    "    display(HTML(tabulate.tabulate(table, tablefmt='html')))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacfee67",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-08T08:28:18.011086Z",
     "start_time": "2021-12-08T08:28:13.130084Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "names = ['LastFM', 'MOOC', 'Reddit', 'Wikipedia']\n",
    "for name in names:\n",
    "    graph_statistics(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de63771f",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4a9831",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-10T14:57:14.392828Z",
     "start_time": "2021-12-10T14:57:14.349831Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_results(test, data_name):\n",
    "    col_names = {\"TGN \\n(no time)\": ['tgn_tgn_ablation_time_pred_run_0', 'tgn_tgn_ablation_time_pred_run_1',\n",
    "                                               'tgn_tgn_ablation_time_pred_run_2', 'tgn_tgn_ablation_time_pred_run_3',\n",
    "                                               'tgn_tgn_ablation_time_pred_run_4'],\n",
    "                \"TGN\": ['tgn_tgn_pred_run_0',\n",
    "                       'tgn_tgn_pred_run_1', 'tgn_tgn_pred_run_2', 'tgn_tgn_pred_run_3',\n",
    "                       'tgn_tgn_pred_run_4'],\n",
    "                \"Jodie \\n(no time)\": ['jodie_jodie_ablation_time_pred_run_0',\n",
    "                                               'jodie_jodie_ablation_time_pred_run_1',\n",
    "                                               'jodie_jodie_ablation_time_pred_run_2',\n",
    "                                               'jodie_jodie_ablation_time_pred_run_3',\n",
    "                                               'jodie_jodie_ablation_time_pred_run_4'],\n",
    "                \"Jodie\": ['jodie_jodie_pred_run_0',\n",
    "                           'jodie_jodie_pred_run_1', 'jodie_jodie_pred_run_2',\n",
    "                           'jodie_jodie_pred_run_3', 'jodie_jodie_pred_run_4'],\n",
    "                \"DyRep \\n(no time)\": ['dyrep_dyrep_ablation_time_pred_run_0',\n",
    "                                               'dyrep_dyrep_ablation_time_pred_run_1',\n",
    "                                               'dyrep_dyrep_ablation_time_pred_run_2',\n",
    "                                               'dyrep_dyrep_ablation_time_pred_run_3',\n",
    "                                               'dyrep_dyrep_ablation_time_pred_run_4'],\n",
    "                \"DyRep\": ['dyrep_dyrep_pred_run_0',\n",
    "                           'dyrep_dyrep_pred_run_1', 'dyrep_dyrep_pred_run_2',\n",
    "                           'dyrep_dyrep_pred_run_3', 'dyrep_dyrep_pred_run_4'],\n",
    "                \"Pref. \\nAttach.\":['pa'],\n",
    "                \"Neigh. \\nMeasure\":['nm'],\n",
    "                \"Katz\":['ra'],\n",
    "                \"Algebraic \\nDist.\":['dist']\n",
    "                #\"Common Ego Edges\": ['cee']\n",
    "                }\n",
    "    \n",
    "    rocauc_scores = pd.DataFrame(columns=[\"model\", \"auc\"])\n",
    "    prauc_scores = pd.DataFrame(columns=[\"model\", \"auc\"])\n",
    "    \n",
    "    df = test.copy()\n",
    "    \n",
    "    for col_pred in ['pa', 'nm', 'ra', 'dist']:\n",
    "        scaler = MinMaxScaler()\n",
    "        df[col_pred] = scaler.fit_transform(test[[col_pred]])\n",
    "        df[col_pred].fillna(0, inplace=True)\n",
    "\n",
    "    \n",
    "    label = test.ground_truth.values\n",
    "    for title, col in col_names.items():\n",
    "        fig, [ax_1, ax_2] = plt.subplots(1,2, figsize=(12,4))\n",
    "        tprs = []\n",
    "        base_fpr = np.linspace(0, 1, 101)\n",
    "        auc=[]\n",
    "        for pred in col:\n",
    "            mask = (~df[pred].isna())\n",
    "            print(f\"Skipping {df[df[pred].isna()].shape[0]} new edges...\")\n",
    "            pred_prob = df[mask][pred].values\n",
    "            label_col = df[mask].ground_truth.values\n",
    "            fpr, tpr, thresh = metrics.roc_curve(label_col,\n",
    "                                                 pred_prob)\n",
    "            ax_1.plot(fpr, tpr, color=[52/255,97/255,120/255], alpha=0.15)\n",
    "            a = metrics.roc_auc_score(label_col, pred_prob)\n",
    "            auc.append(a)\n",
    "            rocauc_scores = rocauc_scores.append({\"model\":title, \"auc\":a}, ignore_index=True)\n",
    "            \n",
    "            tpr = interp(base_fpr, fpr, tpr)\n",
    "            tpr[0] = 0.0\n",
    "            tprs.append(tpr)\n",
    "        tprs = np.array(tprs)\n",
    "        mean_tprs = tprs.mean(axis=0)\n",
    "        std = tprs.std(axis=0)\n",
    "            \n",
    "        tprs_upper = np.minimum(mean_tprs + std, 1)\n",
    "        tprs_lower = mean_tprs - std\n",
    "        \n",
    "        ax_1.plot(base_fpr, mean_tprs, color=[52/255,97/255,120/255], lw=2)\n",
    "        ax_1.fill_between(base_fpr, tprs_lower, tprs_upper, color=[143/255,195/255,216/255], alpha=0.3)\n",
    "\n",
    "        ax_1.plot([0, 1], [0, 1],'--', color = [252/255,97/255,31/255])\n",
    "        ax_1.set_xlim([-0.01, 1.01])\n",
    "        ax_1.set_ylim([-0.01, 1.01])\n",
    "        ax_1.set_ylabel('True Positive Rate')\n",
    "        ax_1.set_xlabel('False Positive Rate')\n",
    "        # plt.axes().set_aspect('equal', 'datalim')\n",
    "        ax_1.set_title(f\"{title} ROC curve, AUC: {mean(auc):.4f}\")\n",
    "\n",
    "        # PR curve\n",
    "        tprs = []\n",
    "        base_fpr = np.linspace(0, 1, 101)\n",
    "        auc=[]\n",
    "        for pred in col:\n",
    "            mask = (~df[pred].isna())\n",
    "            print(f\"Skipping {df[df[pred].isna()].shape[0]} new edges...\")\n",
    "            pred_prob = df[mask][pred].values\n",
    "            label_col = df[mask].ground_truth.values\n",
    "            precision, recall, thresholds = metrics.precision_recall_curve(label_col, pred_prob)\n",
    "            a = metrics.auc(recall, precision)\n",
    "            auc.append(a)\n",
    "            prauc_scores = prauc_scores.append({\"model\":title, \"auc\":a}, ignore_index=True)\n",
    "            \n",
    "            ax_2.plot(recall, precision, color=[52/255,97/255,120/255], alpha=0.15)\n",
    "            \n",
    "            reversed_recall = np.fliplr([recall])[0]\n",
    "            reversed_precision = np.fliplr([precision])[0]\n",
    "            tpr = interp(base_fpr, reversed_recall, reversed_precision)\n",
    "            tpr[0] = 1.0\n",
    "            tprs.append(tpr)\n",
    "        tprs = np.array(tprs)\n",
    "        mean_tprs = tprs.mean(axis=0)\n",
    "        std = tprs.std(axis=0)\n",
    "            \n",
    "        tprs_upper = np.minimum(mean_tprs + std, 1)\n",
    "        tprs_lower = mean_tprs - std\n",
    "        \n",
    "        ax_2.plot(base_fpr, mean_tprs, color=[52/255,97/255,120/255], lw=2)\n",
    "        ax_2.fill_between(base_fpr, tprs_lower, tprs_upper, color=[143/255,195/255,216/255], alpha=0.3)\n",
    "        ax_2.plot([0, 1], [0.5, 0.5],'--', color = [252/255,97/255,31/255])\n",
    "        ax_2.set_xlim([-0.01, 1.01])\n",
    "        ax_2.set_ylim([-0.01, 1.01])\n",
    "        ax_2.set_xlabel('Recall')\n",
    "        ax_2.set_ylabel('Precision')\n",
    "        ax_2.set_title(f\"{title} PR curve, AUC: {mean(auc):.4f}\")\n",
    "        plt.show()\n",
    "        \n",
    "        # display(auc_scores)\n",
    "\n",
    "    max_width = 10\n",
    "    \n",
    "    plt.figure(figsize = (7,5))\n",
    "    ax = sns.boxplot(x=\"model\", y=\"auc\",\n",
    "                    data=rocauc_scores,\n",
    "                    linewidth=2.5,\n",
    "                    color=[52/255,97/255,120/255],\n",
    "                    width=0.5)\n",
    "    ax.plot([-0.5, 9.5], [0.5, 0.5],'--', color = [252/255,97/255,31/255])\n",
    "    # ax.set_xticklabels(ax.get_xticklabels(),rotation = 30)\n",
    "    # ax.set_xticklabels(textwrap.fill(x.get_text(), max_width) for x in ax.get_xticklabels())\n",
    "    ax.set_xlabel(\"Model\")\n",
    "    ax.set_ylabel(\"AUC\")\n",
    "    plt.title(f\"ROC AUC scores for {data_name}\",fontsize=14)\n",
    "    plt.savefig(f\"../reports/figures/models_roc_auc_{data_name}.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize = (7,5))\n",
    "    ax = sns.boxplot(x=\"model\", y=\"auc\",\n",
    "                    data=prauc_scores,\n",
    "                    linewidth=2.5,\n",
    "                    color=[52/255,97/255,120/255],\n",
    "                    width=0.5)\n",
    "    ax.plot([-0.5, 9.5], [0.5, 0.5],'--', color = [252/255,97/255,31/255])\n",
    "    # ax.set_xticklabels(ax.get_xticklabels(),rotation = 30)\n",
    "    ax.set_xlabel(\"Model\")\n",
    "    ax.set_ylabel(\"AUC\")\n",
    "    plt.title(f\"PR AUC scores for {data_name}\",fontsize=14)\n",
    "    plt.savefig(f\"../reports/figures/models_pr_auc_{data_name}.png\")\n",
    "    plt.show()\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cdbf54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-10T14:58:48.613829Z",
     "start_time": "2021-12-10T14:57:15.134830Z"
    }
   },
   "outputs": [],
   "source": [
    "for data_name in ['lastfm', 'mooc', 'wikipedia', 'reddit']:\n",
    "    print(data_name)\n",
    "    test = pd.read_csv(os.path.join(\"../data/processed\", f\"{data_name}/test/ml_{data_name}.csv\"), index_col=0)\n",
    "    \n",
    "    # add embedding methods\n",
    "    for root, dirs, files in os.walk(f\"../data/results/{data_name}\"):\n",
    "        if files:\n",
    "            for file in files:\n",
    "                if \"pred\" in file:\n",
    "                    test[os.path.basename(file).split('.')[0]] = np.load(os.path.join(root, file))\n",
    "                    \n",
    "    # add static methods\n",
    "    test_static = pd.read_csv(f\"../data/results/{data_name}/static_result_{data_name}.csv\", index_col=0)\n",
    "    test[['pa', 'nm', 'ra', 'dist']] = test_static[['pa', 'nm', 'ra', 'dist']]\n",
    "    \n",
    "    # test_cee = pd.read_csv(f\"../data/results/{data_name}/static_result_cee_{data_name}.csv\", index_col=0)\n",
    "    # test[['cee']] = test_cee[['cee']]\n",
    "    \n",
    "    df = plot_results(test, data_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cdbe02",
   "metadata": {},
   "source": [
    "# Learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3979f6d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-10T23:37:44.356318Z",
     "start_time": "2021-12-10T23:37:44.331844Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_learning_curve(dataset_name):\n",
    "    col_names = {\n",
    "                \"tgn\": ['tgn_tgn_pred_run_0',\n",
    "                       'tgn_tgn_pred_run_1', 'tgn_tgn_pred_run_2', 'tgn_tgn_pred_run_3',\n",
    "                       'tgn_tgn_pred_run_4'],\n",
    "                \"jodie\": ['jodie_jodie_pred_run_0',\n",
    "                           'jodie_jodie_pred_run_1', 'jodie_jodie_pred_run_2',\n",
    "                           'jodie_jodie_pred_run_3', 'jodie_jodie_pred_run_4'],\n",
    "                \"dyrep\": ['dyrep_dyrep_pred_run_0',\n",
    "                           'dyrep_dyrep_pred_run_1', 'dyrep_dyrep_pred_run_2',\n",
    "                           'dyrep_dyrep_pred_run_3', 'dyrep_dyrep_pred_run_4']\n",
    "                }\n",
    "    \n",
    "    path = \"../data/results\"\n",
    "    scores = pd.DataFrame(columns=['folder', 'model', 'rocauc', 'prauc'])\n",
    "    folders = [x for x in os.listdir(path)if x[:len(f\"{dataset_name}_\")] == f\"{dataset_name}_\"]\n",
    "    for folder in folders:\n",
    "        full_path = os.path.join(path, folder)\n",
    "        test = pd.read_csv(f\"../data/processed/split_data/{dataset_name}/{folder}/test/ml_{dataset_name}.csv\")\n",
    "        for model in os.listdir(full_path):\n",
    "            for pred_file in col_names[model]:\n",
    "                if 'pred' in pred_file:\n",
    "                    pred_path = os.path.join(full_path,model, pred_file)\n",
    "                    if not os.path.isfile(pred_path + \".npy\"):\n",
    "                        print(f\"{pred_path} not found\")\n",
    "                        continue\n",
    "                    pred = np.load(pred_path + \".npy\")\n",
    "                    test[f\"{folder}_{model}_{pred_file[-1]}\"] = pred\n",
    "                    label = test.ground_truth \n",
    "                    precision, recall, thresholds = metrics.precision_recall_curve(label,\n",
    "                                                                                   pred)\n",
    "                    prauc = metrics.auc(recall, precision)\n",
    "                    fpr, tpr, thresh = metrics.roc_curve(label,\n",
    "                                                 pred)\n",
    "                    rocauc = metrics.roc_auc_score(label, pred)\n",
    "                    scores = scores.append({'folder':folder,\n",
    "                                            'model':model,\n",
    "                                            'rocauc':rocauc,\n",
    "                                            'prauc':prauc},\n",
    "                                          ignore_index=True)\n",
    "    f = sns.lineplot(x='folder', y='rocauc',\n",
    "                     data=scores,\n",
    "                     hue='model', hue_order = [\"tgn\", \"jodie\", \"dyrep\"],\n",
    "                    palette = customPalette)\n",
    "    plt.legend(['TGN', 'Jodie', 'DyRep'])\n",
    "    plt.title(f\"ROC AUC score development ({dataset_name})\")\n",
    "    f.set_xticklabels([f\"{x}0%\" for x in range(1,11)])\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.ylabel(\"ROC AUC\")\n",
    "    plt.xlabel(\"Ratio of data used\")\n",
    "    plt.show()\n",
    "    \n",
    "    f = sns.lineplot(x='folder', y='prauc',\n",
    "                     data=scores,\n",
    "                     hue='model', hue_order = [\"tgn\", \"jodie\", \"dyrep\"],\n",
    "                     palette = customPalette)\n",
    "    plt.legend(['TGN', 'Jodie', 'DyRep'])\n",
    "    plt.title(f\"PR AUC score development ({dataset_name})\")\n",
    "    f.set_xticklabels([f\"{x}0%\" for x in range(1,11)])\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.ylabel(\"PR AUC\")\n",
    "    plt.xlabel(\"Ratio of data used\")\n",
    "    plt.show()\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a09c15f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-10T23:37:53.491206Z",
     "start_time": "2021-12-10T23:37:45.428243Z"
    }
   },
   "outputs": [],
   "source": [
    "s = plot_learning_curve('wikipedia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b5cdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = plot_learning_curve('reddit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b844ff05",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = plot_learning_curve('mooc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b77bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = plot_learning_curve('lastfm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab89d0e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-10T22:16:54.082307Z",
     "start_time": "2021-12-10T22:16:54.070281Z"
    }
   },
   "source": [
    "# Node2vec comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7004b71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-10T22:26:27.907424Z",
     "start_time": "2021-12-10T22:26:27.872545Z"
    }
   },
   "outputs": [],
   "source": [
    "scores = pd.DataFrame(columns=['dataset', 'model', 'score'])\n",
    "scores['dataset'] = ['Facebook']*8 + ['PPI']*8 + ['arXiv']*8\n",
    "scores['model'] = ['Common Neighbors', 'Jaccardâ€™s Coefficient', 'Adamic-Adar ', 'Pref. Attachment', 'Spectral Clustering', 'DeepWalk', 'LINE', 'node2vec']*3\n",
    "scores['score'] = [0.8100,\n",
    "                  0.8880,\n",
    "                  0.8289,\n",
    "                  0.7137,\n",
    "                  0.7200,\n",
    "                  0.9680,\n",
    "                  0.9490,\n",
    "                  0.9680,\n",
    "                  0.7142,\n",
    "                  0.7018,\n",
    "                  0.7126,\n",
    "                  0.6670,\n",
    "                  0.6588,\n",
    "                  0.7441,\n",
    "                  0.7249,\n",
    "                  0.7719,\n",
    "                  0.8153,\n",
    "                  0.8067,\n",
    "                  0.8315,\n",
    "                  0.6996,\n",
    "                  0.7099,\n",
    "                  0.9340,\n",
    "                  0.8902,\n",
    "                  0.9366]\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00769695",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-10T23:16:20.983744Z",
     "start_time": "2021-12-10T23:16:20.187288Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (6,6))\n",
    "f = sns.catplot(x='dataset', y='score',\n",
    "               data=scores,\n",
    "               hue='model',\n",
    "               palette=customPalette,\n",
    "               kind='swarm',\n",
    "               s=10)\n",
    "\n",
    "plt.ylabel(\"AUC score\")\n",
    "plt.xlabel(\"Dataset name\")\n",
    "f._legend.set_title(\"Method\")\n",
    "#plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f7a02a",
   "metadata": {},
   "source": [
    "# Jodie results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21855ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-12T10:13:03.566038Z",
     "start_time": "2021-12-12T10:13:01.385041Z"
    }
   },
   "outputs": [],
   "source": [
    "scores = pd.DataFrame(columns=['dataset', 'model', 'score'])\n",
    "scores['dataset'] = ['Reddit']*6 + ['Wikipedia']*6 + ['MOOC']*6\n",
    "scores['model'] = ['LSTM', 'TIME-LSTM', 'RNN', 'LatentCross', 'DeepCoevolve', 'JODIE']*3\n",
    "scores['score'] = [0.523,0.556,0.586,0.574,0.577,0.599,0.575,0.671,0.804,0.628,0.663,0.831,0.686,0.711,0.558,0.686,0.671,0.756]\n",
    "\n",
    "plt.figure(figsize = (6,6))\n",
    "f = sns.catplot(x='dataset', y='score',\n",
    "               data=scores,\n",
    "               hue='model',\n",
    "               palette=customPalette,\n",
    "               kind='swarm',\n",
    "               s=10)\n",
    "\n",
    "plt.ylabel(\"AUC score\")\n",
    "plt.xlabel(\"Dataset name\")\n",
    "f._legend.set_title(\"Method\")\n",
    "#plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9477db",
   "metadata": {},
   "source": [
    "# TGN results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ec11be",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.DataFrame(columns=['dataset', 'model', 'score'])\n",
    "scores['dataset'] = ['Wikipedia']*5 + ['Reddit']*5\n",
    "scores['model'] = ['DeepWalk', 'Node2Vec', 'Jodie', 'DyRep', 'TGN']*2\n",
    "scores['score'] = [90.71, 91.48, 94.62, 94.59, 98.46, 83.10, 84.58,97.11,97.98, 98.7]\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize = (3,3))\n",
    "f = sns.catplot(x='dataset', y='score',\n",
    "               data=scores,\n",
    "               hue='model',\n",
    "               palette=customPalette,\n",
    "               kind='swarm',\n",
    "               s=10)\n",
    "\n",
    "plt.ylabel(\"Average precision\")\n",
    "plt.xlabel(\"Dataset name\")\n",
    "f._legend.set_title(\"Method\")\n",
    "#plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.savefig(f\"../reports/figures/original_tgn_prec.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef69e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.DataFrame(columns=['dataset', 'model', 'score'])\n",
    "scores['dataset'] = ['Wikipedia']*3 + ['Reddit']*3\n",
    "scores['model'] = ['Jodie', 'DyRep', 'TGN']*2\n",
    "scores['score'] = [85.84,84.59,87.81, 61.83,62.91, 67.06]\n",
    "\n",
    "plt.figure(figsize = (3,3))\n",
    "f = sns.catplot(x='dataset', y='score',\n",
    "               data=scores,\n",
    "               hue='model',\n",
    "               palette=customPalette,\n",
    "               kind='swarm',\n",
    "               s=10)\n",
    "\n",
    "plt.ylabel(\"ROC AUC\")\n",
    "plt.xlabel(\"Dataset name\")\n",
    "f._legend.set_title(\"Method\")\n",
    "#plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.savefig(f\"../reports/figures/original_tgn_auc.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecde1f8",
   "metadata": {},
   "source": [
    "# Node embedding viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b7753d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What to visulize?\n",
    "data = pd.read_csv(\"../data/interim/mooc/ml_mooc.csv\", index_col=0)\n",
    "plt.hist(data.i.value_counts())\n",
    "\n",
    "plt.show()\n",
    "plt.hist(data.u.value_counts())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51ce94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['u', 'i']].value_counts().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7d7dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = data.i.value_counts()\n",
    "mask = (d>2500) & (d<5000)\n",
    "items = d[mask].index.tolist()\n",
    "\n",
    "d = data.u.value_counts()\n",
    "mask = (d>50) & (d<100)\n",
    "users = d[mask].index.tolist()\n",
    "\n",
    "display(data.query(\"u in  @users and i in @items\"))\n",
    "display(data.query(\"u in  @users or i in @items\"))\n",
    "display(data.query(\"u in  @users and i in @items\")[['u', 'i']].value_counts().sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fde814c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# best model \n",
    "test = pd.read_csv(\"../data/processed/mooc/test/ml_mooc.csv\")\n",
    "res = [\"../data/results/mooc/tgn/tgn_tgn_pred_run_0.npy\",\n",
    "        \"../data/results/mooc/tgn/tgn_tgn_pred_run_1.npy\",\n",
    "        \"../data/results/mooc/tgn/tgn_tgn_pred_run_2.npy\",\n",
    "        \"../data/results/mooc/tgn/tgn_tgn_pred_run_3.npy\",\n",
    "        \"../data/results/mooc/tgn/tgn_tgn_pred_run_4.npy\"]\n",
    "\n",
    "scores = []\n",
    "for r in res:\n",
    "    pred = np.load(r)\n",
    "    label = test.ground_truth\n",
    "    scores.append(roc_auc_score(label, pred))\n",
    "    print(roc_auc_score(label, pred))\n",
    "\n",
    "print()\n",
    "print(sum(scores)/len(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55fd2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best model\n",
    "models_to_load = {0: \"../models/mooc/tgn/mooc-tgn.pth\"}\n",
    "models_to_load_2 = {0: \"../models/mooc/tgn/mooc-tgn_viz_extra_layers_had_tanh.pth\"}\n",
    "models_to_load_3 = {0: \"../models/mooc/tgn/mooc-tgn_viz_extra_layers_extra6.pth\"}\n",
    "\n",
    "\n",
    "model = tgn_viz.predict(\"mooc\", \"tgn\", ablation=None, seed=0,\n",
    "            n_runs=5, n_epoch=50,\n",
    "            data_path = f\"../data\",\n",
    "            models_to_load=models_to_load,\n",
    "            affinity_merge_layer=\"default\",\n",
    "            return_loaded_model=True,\n",
    "            memory_dim = 172,\n",
    "            node_feature_size = 172)\n",
    "\n",
    "model2 = tgn_viz.predict(\"mooc\", \"tgn\", ablation=None, seed=0,\n",
    "            n_runs=5, n_epoch=50,\n",
    "            data_path = f\"../data\",\n",
    "            models_to_load=models_to_load_2,\n",
    "            affinity_merge_layer=\"extra_layers_had_tanh\",\n",
    "            return_loaded_model=True,\n",
    "            memory_dim = 2,\n",
    "            node_feature_size = 2)\n",
    "\n",
    "model3 = tgn_viz.predict(\"mooc\", \"tgn\", ablation=None, seed=0,\n",
    "            n_runs=5, n_epoch=50,\n",
    "            data_path = f\"../data\",\n",
    "            models_to_load=models_to_load_3,\n",
    "            affinity_merge_layer=\"extra_layers_extra6\",\n",
    "            return_loaded_model=True,\n",
    "            memory_dim = 2,\n",
    "            node_feature_size = 2)\n",
    "\n",
    "memory1 = model.memory.backup_memory()\n",
    "memory2 = model2.memory.backup_memory()\n",
    "memory3 = model3.memory.backup_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fdd36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tgn.utils.utils import get_neighbor_finder\n",
    "dataset_name = \"mooc\"\n",
    "\n",
    "data_full_path = os.path.join(\"../data/interim/mooc\")\n",
    "\n",
    "full_graph_df = pd.read_csv(os.path.join(data_full_path, f\"ml_{dataset_name}.csv\"))\n",
    "full_edge_features = np.load(os.path.join(data_full_path, f\"ml_{dataset_name}.npy\"))\n",
    "full_node_features = np.load(os.path.join(data_full_path, f\"ml_{dataset_name}_node.npy\"))\n",
    "\n",
    "full_sources = full_graph_df.u.values\n",
    "full_destinations = full_graph_df.i.values\n",
    "full_edge_idxs = full_graph_df.idx.values\n",
    "full_labels = full_graph_df.label.values\n",
    "full_timestamps = full_graph_df.ts.values\n",
    "\n",
    "full_data = Data(full_sources,\n",
    "                full_destinations,\n",
    "                full_timestamps,\n",
    "                full_edge_idxs,\n",
    "                full_labels)\n",
    "\n",
    "full_ngh_finder = get_neighbor_finder(full_data, False)\n",
    "model.embedding_module.neighbor_finder = full_ngh_finder\n",
    "model2.embedding_module.neighbor_finder = full_ngh_finder\n",
    "model3.embedding_module.neighbor_finder = full_ngh_finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740f36ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model AUC\n",
    "data_true = pd.read_csv(\"../data/processed/mooc/test/ml_mooc_true.csv\", index_col=0)\n",
    "data_true = data_true[data_true.i < 7135]\n",
    "test_sources = data_true.u.values\n",
    "test_destinations = data_true.i.values\n",
    "test_edge_idxs = data_true.idx.values\n",
    "test_labels = data_true.label.values\n",
    "test_timestamps = data_true.ts.values\n",
    "\n",
    "test_data_true = Data(test_sources,\n",
    "                   test_destinations,\n",
    "                   test_timestamps,\n",
    "                   test_edge_idxs,\n",
    "                   test_labels)\n",
    "\n",
    "data_false = pd.read_csv(\"../data/processed/mooc/test/ml_mooc_false.csv\", index_col=0)\n",
    "data_false = data_false[data_false.i < 7135]\n",
    "test_sources = data_false.u.values\n",
    "test_destinations = data_false.i.values\n",
    "test_edge_idxs = data_false.idx.values\n",
    "test_labels = data_false.label.values\n",
    "test_timestamps = data_false.ts.values\n",
    "\n",
    "test_data_false = Data(test_sources,\n",
    "                   test_destinations,\n",
    "                   test_timestamps,\n",
    "                   test_edge_idxs,\n",
    "                   test_labels)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "res_t = tgn_viz.predict_links(model, test_data_true, 10, batch_size=200, embed_size=172)\n",
    "model.memory.restore_memory(memory1)\n",
    "res_f = tgn_viz.predict_links(model, test_data_false, 10, batch_size=200, embed_size=172, update_with_positives=False)\n",
    "pred_score = np.concatenate([res_t[0], res_f[0]])\n",
    "true_label = np.concatenate([np.ones(res_t[0].shape[0]),\n",
    "                            np.zeros(res_f[0].shape[0])])\n",
    "print(roc_auc_score(true_label, pred_score))\n",
    "model.memory.restore_memory(memory1)\n",
    "\n",
    "model2.eval()\n",
    "res_t = tgn_viz.predict_links(model2, test_data_true, 10, batch_size=200, embed_size=2)\n",
    "model2.memory.restore_memory(memory2)\n",
    "res_f = tgn_viz.predict_links(model2, test_data_false, 10, batch_size=200, embed_size=2, update_with_positives=False)\n",
    "pred_score = np.concatenate([res_t[0], res_f[0]])\n",
    "true_label = np.concatenate([np.ones(res_t[0].shape[0]),\n",
    "                            np.zeros(res_f[0].shape[0])])\n",
    "print(roc_auc_score(true_label, pred_score))\n",
    "model2.memory.restore_memory(memory2)\n",
    "\n",
    "model3.eval()\n",
    "\n",
    "res_t = tgn_viz.predict_links(model3, test_data_true, 10, batch_size=200, embed_size=2)\n",
    "model3.memory.restore_memory(memory3)\n",
    "res_f = tgn_viz.predict_links(model3, test_data_false, 10, batch_size=200, embed_size=2, update_with_positives=False)\n",
    "pred_score = np.concatenate([res_t[0], res_f[0]])\n",
    "true_label = np.concatenate([np.ones(res_t[0].shape[0]),\n",
    "                            np.zeros(res_f[0].shape[0])])\n",
    "print(roc_auc_score(true_label, pred_score))\n",
    "model3.memory.restore_memory(memory3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2af6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_pairs = np.array([[2134,  7066],\n",
    "        [4857,  7066],\n",
    "        [3958,  7058],\n",
    "        [4309,  7058],\n",
    "        [1046,  7058],\n",
    "        [6419,  7079],\n",
    "        [4861,  7079],\n",
    "        [4386,  7082],\n",
    "        [6680,  7082],\n",
    "        [6629,  7075]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6913cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict all\n",
    "\n",
    "data = pd.read_csv(\"../data/interim/mooc/ml_mooc.csv\", index_col=0)\n",
    "test_sources = data.u.values\n",
    "test_destinations = data.i.values\n",
    "test_edge_idxs = data.idx.values\n",
    "test_labels = data.label.values\n",
    "test_timestamps = data.ts.values\n",
    "\n",
    "test_data_true = Data(test_sources,\n",
    "                   test_destinations,\n",
    "                   test_timestamps,\n",
    "                   test_edge_idxs,\n",
    "                   test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d732ba72",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.memory.__init_memory__()\n",
    "model2.memory.__init_memory__()\n",
    "model3.memory.__init_memory__()\n",
    "\n",
    "res = tgn_viz.predict_links(model, test_data_true, 10, batch_size=200, embed_size=172)\n",
    "\n",
    "res2 = tgn_viz.predict_links(model2, test_data_true, 10, batch_size=200, embed_size=2)\n",
    "\n",
    "res3 = tgn_viz.predict_links(model3, test_data_true, 10, batch_size=200, embed_size=2)\n",
    "\n",
    "model.memory.restore_memory(memory1)\n",
    "model2.memory.restore_memory(memory2)\n",
    "model3.memory.restore_memory(memory3)\n",
    "\n",
    "preds, (nodes, times, embeds) = res\n",
    "preds2, (nodes2, times2, embeds2) = res2\n",
    "preds3, (nodes3, times3, embeds3) = res3\n",
    "\n",
    "sc = StandardScaler()\n",
    "sc.fit(embeds)\n",
    "X_train_std = sc.transform(embeds)\n",
    "pca = PCA(n_components=2)\n",
    "embed_pca = pca.fit_transform(X_train_std)\n",
    "\n",
    "sc2 = StandardScaler()\n",
    "sc2.fit(embeds2)\n",
    "X_train_std2 = sc2.transform(embeds2)\n",
    "pca2 = PCA(n_components=2)\n",
    "embed_pca2 = pca2.fit_transform(X_train_std2)\n",
    "\n",
    "sc3 = StandardScaler()\n",
    "sc3.fit(embeds3)\n",
    "X_train_std3 = sc3.transform(embeds3)\n",
    "pca3 = PCA(n_components=2)\n",
    "embed_pca3 = pca3.fit_transform(X_train_std3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ed141b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "sc = StandardScaler()\n",
    "sc.fit(embeds)\n",
    "X_train_std = sc.transform(embeds)\n",
    "pca = PCA(n_components=4)\n",
    "X_train_pca = pca.fit_transform(X_train_std)\n",
    "exp_var_pca = pca.explained_variance_ratio_\n",
    "cum_sum_eigenvalues = np.cumsum(exp_var_pca)\n",
    "\n",
    "plt.bar(range(0,len(exp_var_pca)), exp_var_pca, alpha=0.5, align='center', label='Individual explained variance')\n",
    "plt.step(range(0,len(cum_sum_eigenvalues)), cum_sum_eigenvalues, where='mid',label='Cumulative explained variance')\n",
    "plt.ylabel('Explained variance ratio', fontsize=14)\n",
    "plt.xlabel('Principal component index', fontsize=14)\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.xticks(list(range(4)))\n",
    "plt.show()\n",
    "\n",
    "sc = StandardScaler()\n",
    "sc.fit(embeds)\n",
    "X_train_std = sc.transform(embeds)\n",
    "pca = PCA(n_components=6)\n",
    "X_train_pca = pca.fit_transform(X_train_std)\n",
    "exp_var_pca = pca.explained_variance_ratio_\n",
    "cum_sum_eigenvalues = np.cumsum(exp_var_pca)\n",
    "\n",
    "plt.bar(range(0,len(exp_var_pca)), exp_var_pca, alpha=0.5, align='center', label='Individual explained variance')\n",
    "plt.step(range(0,len(cum_sum_eigenvalues)), cum_sum_eigenvalues, where='mid',label='Cumulative explained variance')\n",
    "plt.ylabel('Explained variance ratio', fontsize=14)\n",
    "plt.xlabel('Principal component index', fontsize=14)\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.xticks(list(range(6)))\n",
    "plt.savefig(f\"../reports/figures/tgn_172_pca.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bfa286",
   "metadata": {},
   "source": [
    "## Plot node movement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8128db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_min = min(times)\n",
    "y_max = max(times)\n",
    "\n",
    "plt.scatter(x=embed_pca[:,0],\n",
    "            y=embed_pca[:,1],\n",
    "            c=times,\n",
    "            cmap=new_cmap,\n",
    "            alpha=0.05)\n",
    "cmap = plt.get_cmap(new_cmap)\n",
    "norm = plt.Normalize(y_min,y_max)\n",
    "sm =  ScalarMappable(norm=norm, cmap=cmap)\n",
    "sm.set_array([])\n",
    "cbar = plt.colorbar(sm)\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(x=embed_pca2[:,0],\n",
    "            y=embed_pca2[:,1],\n",
    "            c=times2,\n",
    "            cmap=new_cmap,\n",
    "            alpha=0.05)\n",
    "cmap = plt.get_cmap(new_cmap)\n",
    "norm = plt.Normalize(y_min,y_max)\n",
    "sm =  ScalarMappable(norm=norm, cmap=cmap)\n",
    "sm.set_array([])\n",
    "cbar = plt.colorbar(sm)\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(x=embed_pca2[:,0],\n",
    "            y=embed_pca2[:,1],\n",
    "            c=times2,\n",
    "            cmap=new_cmap,\n",
    "            alpha=0.05)\n",
    "cmap = plt.get_cmap(new_cmap)\n",
    "norm = plt.Normalize(y_min,y_max)\n",
    "sm =  ScalarMappable(norm=norm, cmap=cmap)\n",
    "sm.set_array([])\n",
    "cbar = plt.colorbar(sm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84399d25",
   "metadata": {},
   "source": [
    "## Plot user-item pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1710518",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_pairs = np.array([[2134,  7066],\n",
    "        [4857,  7066],\n",
    "        [3958,  7058],\n",
    "        [4309,  7058],\n",
    "        [1046,  7058],\n",
    "        [6419,  7079],\n",
    "        [4861,  7079],\n",
    "        [4386,  7082],\n",
    "        [6680,  7082],\n",
    "        [6629,  7075]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a38e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = pd.read_csv(\"../data/interim/mooc/ml_mooc.csv\", index_col=0)\n",
    "users = node_pairs[:,0]\n",
    "items = node_pairs[:,1]\n",
    "data = data.query(\"u in @users or i in  @items\")\n",
    "\n",
    "test_sources = data.u.values\n",
    "test_destinations = data.i.values\n",
    "test_edge_idxs = data.idx.values\n",
    "test_labels = data.label.values\n",
    "test_timestamps = data.ts.values\n",
    "\n",
    "test_data_true = Data(test_sources,\n",
    "                   test_destinations,\n",
    "                   test_timestamps,\n",
    "                   test_edge_idxs,\n",
    "                   test_labels)\n",
    "model.memory.__init_memory__()\n",
    "model2.memory.__init_memory__()\n",
    "model3.memory.__init_memory__()\n",
    "\n",
    "res = tgn_viz.predict_links(model, test_data_true, 10, batch_size=200, embed_size=172)\n",
    "\n",
    "res2 = tgn_viz.predict_links(model2, test_data_true, 10, batch_size=200, embed_size=2)\n",
    "\n",
    "res3 = tgn_viz.predict_links(model3, test_data_true, 10, batch_size=200, embed_size=2)\n",
    "\n",
    "preds, (nodes, times, embeds) = res\n",
    "preds2, (nodes2, times2, embeds2) = res2\n",
    "preds3, (nodes3, times3, embeds3) = res3\n",
    "\n",
    "sc = StandardScaler()\n",
    "sc.fit(embeds)\n",
    "X_train_std = sc.transform(embeds)\n",
    "pca = PCA(n_components=2)\n",
    "embed_pca = pca.fit_transform(X_train_std)\n",
    "\n",
    "sc2 = StandardScaler()\n",
    "sc2.fit(embeds2)\n",
    "X_train_std2 = sc2.transform(embeds2)\n",
    "pca2 = PCA(n_components=2)\n",
    "embed_pca2 = pca2.fit_transform(X_train_std2)\n",
    "\n",
    "sc3 = StandardScaler()\n",
    "sc3.fit(embeds3)\n",
    "X_train_std3 = sc3.transform(embeds3)\n",
    "pca3 = PCA(n_components=2)\n",
    "embed_pca3 = pca3.fit_transform(X_train_std3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb00be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_min = min(times)\n",
    "y_max = max(times)\n",
    "\n",
    "plt.scatter(x=embed_pca[:,0],\n",
    "            y=embed_pca[:,1],\n",
    "            c=times,\n",
    "            cmap=new_cmap,\n",
    "            alpha=0.05)\n",
    "cmap = plt.get_cmap(new_cmap)\n",
    "norm = plt.Normalize(y_min,y_max)\n",
    "sm =  ScalarMappable(norm=norm, cmap=cmap)\n",
    "sm.set_array([])\n",
    "cbar = plt.colorbar(sm)\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(x=embed_pca2[:,0],\n",
    "            y=embed_pca2[:,1],\n",
    "            c=times2,\n",
    "            cmap=new_cmap,\n",
    "            alpha=0.1)\n",
    "cmap = plt.get_cmap(new_cmap)\n",
    "norm = plt.Normalize(y_min,y_max)\n",
    "sm =  ScalarMappable(norm=norm, cmap=cmap)\n",
    "sm.set_array([])\n",
    "cbar = plt.colorbar(sm)\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(x=embed_pca2[:,0],\n",
    "            y=embed_pca2[:,1],\n",
    "            c=times2,\n",
    "            cmap=new_cmap,\n",
    "            alpha=0.1)\n",
    "cmap = plt.get_cmap(new_cmap)\n",
    "norm = plt.Normalize(y_min,y_max)\n",
    "sm =  ScalarMappable(norm=norm, cmap=cmap)\n",
    "sm.set_array([])\n",
    "cbar = plt.colorbar(sm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aeee0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_embeds(user, item, preds, nodes, times, embeds_pca, title):\n",
    "    user_mask = nodes == user\n",
    "    item_mask = nodes == item\n",
    "\n",
    "    fig, [ax_1, ax_2] = plt.subplots(1,2, figsize=(12,4), sharex=True, sharey=True)\n",
    "\n",
    "    y_min = min(times)\n",
    "    y_max = max(times)\n",
    "\n",
    "    a1 = sns.scatterplot(embeds_pca[user_mask,0],\n",
    "                        embeds_pca[user_mask,1],\n",
    "                        c=times[user_mask],\n",
    "                        alpha=0.5,\n",
    "                        ax=ax_1,cmap=new_cmap,\n",
    "                        vmin=y_min,\n",
    "                        vmax=y_max)\n",
    "\n",
    "    a1.set_title(f\"USER: {user}\")\n",
    "    a2 = sns.scatterplot(embeds_pca[item_mask,0],\n",
    "                        embeds_pca[item_mask,1],\n",
    "                        c=times[item_mask],\n",
    "                        alpha=0.5,\n",
    "                        ax=ax_2,\n",
    "                        cmap=new_cmap,\n",
    "                        vmin=y_min,\n",
    "                        vmax=y_max)\n",
    "    a2.set_title(f\"ITEM: {item}\")\n",
    "\n",
    "    fig.suptitle(title)\n",
    "\n",
    "\n",
    "    cmap = plt.get_cmap(new_cmap)\n",
    "    norm = plt.Normalize(y_min,y_max)\n",
    "    sm =  ScalarMappable(norm=norm, cmap=cmap)\n",
    "    sm.set_array([])\n",
    "    cbar = fig.colorbar(sm, ax=ax_2)\n",
    "    plt.savefig(f\"../reports/figures/{'_'.join(title.lower().split())}_user{user}_item{item}.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9dd3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "for user, item in node_pairs:\n",
    "    print(user, item)\n",
    "    plot_embeds(user, item, preds, nodes, times, embed_pca, \"Original model with reduced dimesnsions\")\n",
    "    plot_embeds(user, item, preds2, nodes2, times2, embed_pca2, \"Model with 2D node embedding\")\n",
    "    #plot_embeds(user, item, preds3, nodes3, times3, embed_pca3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d790b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73035824",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
